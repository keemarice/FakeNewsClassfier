{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "import numpy as np\n", "import matplotlib.pyplot as plt"]}, {"cell_type": "markdown", "metadata": {}, "source": ["ead in data (NEED TO TALK ABT THIS)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["twitter = pd.read_csv('FakeNewsNet.csv') #ONLY INCLUDES TITLES OF ARTICLES\n", "#print(twitter.head(10))\n", "#twitter = twitter.drop(['news_url', 'source_domain', 'tweet_num'], axis=1)\n", "print(twitter.head(10)) \n", "print(twitter.columns)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["ead in albanian data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["albanian = pd.read_csv('alb-fake-news-corpus.csv')\n", "print(albanian.head(10))\n", "print(albanian.columns)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["ead and clean soccer data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fakeSoccer = pd.read_csv('fake-soccer.csv')\n", "realSoccer = pd.read_csv('real-soccer.csv')\n", "fakeSoccer['real'] = 0\n", "realSoccer['real'] = 1\n", "soccer = pd.concat([fakeSoccer, realSoccer])\n", "soccer['tweet'] = soccer['tweet'].fillna(\"\").astype(str) # Fill NaNs and convert to string fixes errors\n", "print(soccer.head(10))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["tart naive bayes"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.feature_extraction.text import CountVectorizer\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.naive_bayes import MultinomialNB\n", "from sklearn.metrics import accuracy_score\n", "import re\n", "from collections import Counter\n", "from sklearn.metrics import classification_report"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def tokenize(text):\n", "    return re.findall(r'\\b\\w+\\b|[!?.,;]', text.lower())  "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def naive_bayes_classifier_count(data, content_col, label_col, test_size=0.2, random_state=42, top_n=10):\n", "    data['tokens'] = data[content_col].apply(tokenize)\n\n", "    # real/fake tokens\n", "    fake_tokens = data[data[label_col] == 1]['tokens'].explode()\n", "    real_tokens = data[data[label_col] == 0]['tokens'].explode()\n", "    fake_word_counts = Counter(fake_tokens)\n", "    real_word_counts = Counter(real_tokens)\n", "    total_fake_words = sum(fake_word_counts.values())\n", "    total_real_words = sum(real_word_counts.values())\n", "    vocab = set(fake_word_counts.keys()).union(set(real_word_counts.keys()))\n", "    vocab_size = len(vocab)\n\n", "    #laplace smoothing for word likelihoods\n", "    word_likelihoods = {\n", "        word: {\n", "            'fake': (fake_word_counts[word] + 1) / (total_fake_words + vocab_size),\n", "            'real': (real_word_counts[word] + 1) / (total_real_words + vocab_size)\n", "        }\n", "        for word in vocab\n", "    }\n\n", "    #nb prediction\n", "    def predict_nb(content, likelihoods, prior_fake, prior_real):\n", "        tokens = tokenize(content)\n", "        log_prob_fake = np.log(prior_fake)\n", "        log_prob_real = np.log(prior_real)\n", "        for token in tokens:\n", "            if token in likelihoods:\n", "                log_prob_fake += np.log(likelihoods[token]['fake'])\n", "                log_prob_real += np.log(likelihoods[token]['real'])\n", "        return 1 if log_prob_fake > log_prob_real else 0 # 1 is fake, 0 is real\n\n", "    # run classifier\n", "    train, test = train_test_split(data, test_size=test_size, random_state=random_state)\n", "    prior_fake = train[label_col].mean()\n", "    prior_real = 1 - prior_fake\n", "    test['predicted'] = test[content_col].apply(\n", "        predict_nb, args=(word_likelihoods, prior_fake, prior_real)\n", "    )\n", "    accuracy = accuracy_score(test[label_col], test['predicted'])\n", "    print(f\"NB (w/ laplace smoothing Accuracy: {accuracy}\")\n\n", "    # print top words influencing fake news\n", "    word_influence = {\n", "        word: np.log(likelihood['fake']) - np.log(likelihood['real'])\n", "        for word, likelihood in word_likelihoods.items()\n", "    }\n", "    sorted_words = sorted(word_influence.items(), key=lambda x: x[1], reverse=True)\n", "    top_words = sorted_words[:top_n]\n", "    print(\"Top words pulling toward fake news:\")\n", "    for word, influence in top_words:\n", "        print(f\"Word: {word}, Pull: {influence:.4f}\")\n\n", "    #print classification report\n", "    print(\"\\nClassification Report:\")\n", "    print(classification_report(test[label_col], test['predicted']))\n", "    \n", "    # Plot misclassified \n", "    misclassified = test[test[label_col] != test['predicted']]\n", "    plt.figure(figsize=(10, 6))\n", "    plt.hist(misclassified[content_col].apply(len), bins=20, color='red', alpha=0.7, label='Misclassified')\n", "    plt.hist(test[content_col].apply(len), bins=20, color='blue', alpha=0.5, label='All')\n", "    plt.xlabel('Length of Content')\n", "    plt.ylabel('Frequency')\n", "    plt.title('Distribution of Content Lengths for Misclassified Examples')\n", "    plt.legend()\n", "    plt.show()\n", "    \n", "  \n", "    return accuracy, top_words"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def naive_bayes_classifier_tfidf(data, content_col, label_col, test_size=0.2, random_state=42, top_n=10, alpha=1.0):\n", "    # Preprocess text\n", "    data['tokens'] = data[content_col].apply(lambda x: ' '.join(tokenize(x)))\n\n", "    # Split data\n", "    X_train, X_test, y_train, y_test = train_test_split(\n", "        data['tokens'], data[label_col], test_size=test_size, random_state=random_state\n", "    )\n", "    vectorizer = TfidfVectorizer()\n", "    X_train_vec = vectorizer.fit_transform(X_train)\n", "    X_test_vec = vectorizer.transform(X_test)\n", "    nb_model = MultinomialNB(alpha=alpha)\n", "    nb_model.fit(X_train_vec, y_train)\n", "    y_pred = nb_model.predict(X_test_vec)\n", "    accuracy = accuracy_score(y_test, y_pred)\n", "    print(f\"Naive Bayes Accuracy (with alpha={alpha}): {accuracy:.2f}\")\n", "    print(\"\\nClassification Report:\")\n", "    print(classification_report(y_test, y_pred))\n", "    feature_names = vectorizer.get_feature_names_out()\n", "    feature_log_probs = nb_model.feature_log_prob_[1] - nb_model.feature_log_prob_[0]\n", "    top_indices = feature_log_probs.argsort()[::-1][:top_n]\n", "    top_words = [(feature_names[i], feature_log_probs[i]) for i in top_indices]\n", "    print(f\"\\nTop {top_n} Influential Words:\")\n", "    for word, score in top_words:\n", "        print(f\"Word: {word}, Score: {score:.4f}\")\n\n", "    #plot misclassified\n", "    misclassified = data.loc[y_test[y_test != y_pred].index]\n", "    plt.figure(figsize=(10, 6))\n", "    plt.hist(misclassified[content_col].apply(len), bins=20, color='red', alpha=0.7, label='Misclassified')\n", "    plt.hist(data[content_col].apply(len), bins=20, color='blue', alpha=0.5, label='All')\n", "    plt.xlabel('Length of Content')\n", "    plt.ylabel('Frequency')\n", "    plt.title('Distribution of Content Lengths for Misclassified Examples')\n", "    plt.legend()\n", "    plt.show()\n", "    \n", "    return accuracy, top_words"]}, {"cell_type": "markdown", "metadata": {}, "source": ["tart knn"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.neighbors import KNeighborsClassifier\n", "from sklearn.feature_extraction.text import TfidfVectorizer\n", "from sklearn.metrics import classification_report"]}, {"cell_type": "markdown", "metadata": {}, "source": ["URSE OF DIMENTIONALITY"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def knn_classifier(data, content_col, label_col, test_size=0.2, random_state=42, n_neighbors=10, top_n=10):\n", "    data['tokens'] = data[content_col].apply(tokenize)\n", "    data['tokens'] = data['tokens'].apply(lambda x: ' '.join(x))\n", "    X_train, X_test, y_train, y_test = train_test_split(data['tokens'], data[label_col], test_size=test_size, random_state=random_state)\n\n", "    # vectorize text data using TF-IDF useful for KNN \n", "    vectorizer = TfidfVectorizer()\n", "    X_train = vectorizer.fit_transform(X_train)\n", "    X_test = vectorizer.transform(X_test)\n", "    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n", "    knn.fit(X_train, y_train)\n", "    y_pred = knn.predict(X_test)\n", "    accuracy = accuracy_score(y_test, y_pred)\n", "    print(f\"KNN Accuracy: {accuracy:.2f}\")\n", "    print(\"\\nClassification Report:\")\n", "    print(classification_report(y_test, y_pred))\n\n", "    # get influential features\n", "    feature_names = vectorizer.get_feature_names_out()\n", "    feature_scores = X_train.mean(axis=0).A1  \n", "    top_indices = feature_scores.argsort()[::-1][:top_n]  \n", "    top_features = [(feature_names[i], feature_scores[i]) for i in top_indices]\n", "    print(f\"\\nTop {top_n} Influential Features:\")\n", "    for feature, score in top_features:\n", "        print(f\"Feature: {feature}, Score: {score:.4f}\")\n\n", "    #plot \n", "    plt.figure(figsize=(10, 6))\n", "    words, scores = zip(*top_features)\n", "    plt.barh(words, scores, color='skyblue', edgecolor='black')\n", "    plt.gca().invert_yaxis()  # Invert y-axis for descending order\n", "    plt.title(f\"Top {top_n} Influential Features for KNN Classification\")\n", "    plt.xlabel(\"Feature Scores (TF-IDF Mean)\")\n", "    plt.ylabel(\"Words\")\n", "    plt.tight_layout()\n", "    plt.show()\n", "    return accuracy, top_features"]}, {"cell_type": "markdown", "metadata": {}, "source": ["tart svm"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.svm import SVC\n", "from sklearn.preprocessing import StandardScaler"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def svm_classifier(data, content_col, label_col, test_size=0.2, random_state=42):\n", "    data['tokens'] = data[content_col].apply(tokenize)\n", "    data['tokens'] = data['tokens'].apply(lambda x: ' '.join(x))\n", "    X_train, X_test, y_train, y_test = train_test_split(data['tokens'], data[label_col], test_size=test_size, random_state=random_state)\n\n", "    # vectorize text data using TF-IDF\n", "    vectorizer = TfidfVectorizer()\n", "    X_train = vectorizer.fit_transform(X_train)\n", "    X_test = vectorizer.transform(X_test)\n\n", "    # standardize features\n", "    scaler = StandardScaler(with_mean=False)\n", "    X_train = scaler.fit_transform(X_train)\n", "    X_test = scaler.transform(X_test)\n\n", "    # train SVM\n", "    svm = SVC()\n", "    svm.fit(X_train, y_train)\n", "    y_pred = svm.predict(X_test)\n", "    accuracy = accuracy_score(y_test, y_pred)\n", "    print(f\"SVM Accuracy: {accuracy:.2f}\")\n\n", "    #print classification report\n", "    print(\"\\nClassification Report:\")\n", "    print(classification_report(y_test, y_pred))\n", "    from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n", "    cm = confusion_matrix(y_test, y_pred, labels=svm.classes_)\n", "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=svm.classes_)\n", "    disp.plot(cmap='Blues')\n", "    plt.title('Confusion Matrix for SVM')\n", "    plt.show()\n", "    return accuracy"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.tree import DecisionTreeClassifier, export_text\n", "def decision_tree_classifier(data, content_col, label_col, test_size=0.2, random_state=42, top_n=10, max_depth_range=(1, 50)):\n", "    data['tokens'] = data[content_col].apply(tokenize)\n", "    data['tokens'] = data['tokens'].apply(lambda x: ' '.join(x))\n", "    X_train, X_test, y_train, y_test = train_test_split(data['tokens'], data[label_col], test_size=test_size, random_state=random_state)\n", "    vectorizer = TfidfVectorizer()\n", "    X_train = vectorizer.fit_transform(X_train)\n", "    X_test = vectorizer.transform(X_test)\n", "    best_accuracy = 0\n", "    best_max_depth = None\n", "    best_top_features = None\n", "    \n", "    for max_depth in range(max_depth_range[0], max_depth_range[1] + 1):\n", "        dt = DecisionTreeClassifier(random_state=random_state, max_depth=max_depth)\n", "        dt.fit(X_train, y_train)\n", "        y_pred = dt.predict(X_test)\n", "        accuracy = accuracy_score(y_test, y_pred)\n", "        if accuracy > best_accuracy:\n", "            best_accuracy = accuracy\n", "            best_max_depth = max_depth\n", "    print(f\"\\nBest Max Depth: {best_max_depth}\")\n", "    print(f\"Best Accuracy: {best_accuracy:.2f}\")\n", "    print(f\"\\nTop {top_n} Influential Features:\")\n", "    from sklearn.tree import plot_tree\n", "    plt.figure(figsize=(20, 10))\n", "    plot_tree(dt, feature_names=vectorizer.get_feature_names_out(), max_depth=2, filled=True)\n", "    plt.show()\n", "    \n", "    return best_accuracy, best_max_depth, best_top_features"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.feature_extraction.text import TfidfVectorizer"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def random_forest_classifier(data, content_col, label_col, test_size=0.2, random_state=42, top_n=10, n_estimators=100):\n", "   \n", "    data['tokens'] = data[content_col].apply(tokenize)\n", "    data['tokens'] = data['tokens'].apply(lambda x: ' '.join(x))\n", "    X_train, X_test, y_train, y_test = train_test_split(data['tokens'], data[label_col], test_size=test_size, random_state=random_state)\n", "    vectorizer = TfidfVectorizer()\n", "    X_train = vectorizer.fit_transform(X_train)\n", "    X_test = vectorizer.transform(X_test)\n", "    accuracy_list = []  # Store accuracy for each number of estimators\n", "    best_accuracy = 0\n", "    best_n_estimators = None\n", "    feature_importances = None\n", "    for n in range(1, n_estimators + 1):\n", "        rf = RandomForestClassifier(random_state=random_state, n_estimators=n)\n", "        rf.fit(X_train, y_train)\n", "        y_pred = rf.predict(X_test)\n", "        accuracy = accuracy_score(y_test, y_pred)\n", "        accuracy_list.append(accuracy)\n", "        if accuracy > best_accuracy:\n", "            best_accuracy = accuracy\n", "            best_n_estimators = n\n", "            feature_importances = rf.feature_importances_  \n", "    plt.figure(figsize=(8, 6))\n", "    plt.plot(range(1, n_estimators + 1), accuracy_list, marker='o', color='blue', label='Accuracy')\n", "    plt.axvline(x=best_n_estimators, color='red', linestyle='--', label=f'Best Estimators: {best_n_estimators}')\n", "    plt.title('Accuracy vs. Number of Estimators (Random Forest)')\n", "    plt.xlabel('Number of Estimators')\n", "    plt.ylabel('Accuracy')\n", "    plt.legend()\n", "    plt.tight_layout()\n", "    plt.show()\n", "    if feature_importances is not None:\n", "        feature_names = vectorizer.get_feature_names_out()\n", "        top_indices = feature_importances.argsort()[::-1][:top_n]\n", "        top_features = [(feature_names[i], feature_importances[i]) for i in top_indices]\n", "        \n", "        # Bar chart for top features\n", "        plt.figure(figsize=(8, 6))\n", "        feature_names_plot, importance_scores = zip(*top_features)\n", "        plt.barh(feature_names_plot, importance_scores, color='green', alpha=0.7)\n", "        plt.title(f'Top {top_n} Influential Features')\n", "        plt.xlabel('Feature Importance')\n", "        plt.gca().invert_yaxis()  \n", "        plt.tight_layout()\n", "        plt.show()\n", "    else:\n", "        top_features = None\n", "    print(f\"\\nBest Number of Estimators: {best_n_estimators}\")\n", "    print(f\"Best Accuracy: {best_accuracy:.2f}\")\n", "    return best_accuracy, best_n_estimators, top_features"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.linear_model import LogisticRegression"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def logistic_regression_classifier(data, content_col, label_col, test_size=0.2, random_state=42):\n", "    data['tokens'] = data[content_col].apply(tokenize)\n", "    data['tokens'] = data['tokens'].apply(lambda x: ' '.join(x))\n", "    X_train, X_test, y_train, y_test = train_test_split(data['tokens'], data[label_col], test_size=test_size, random_state=random_state)\n", "    vectorizer = TfidfVectorizer()\n", "    X_train = vectorizer.fit_transform(X_train)\n", "    X_test = vectorizer.transform(X_test)\n", "    scaler = StandardScaler(with_mean=False)\n", "    X_train = scaler.fit_transform(X_train)\n", "    X_test = scaler.transform(X_test)\n", "    lr = LogisticRegression()\n", "    lr.fit(X_train, y_train)\n", "    y_pred = lr.predict(X_test)\n", "    accuracy = accuracy_score(y_test, y_pred)\n", "    print(f\"Logistic Regression Accuracy: {accuracy:.2f}\")\n", "    return accuracy"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n", "def lda_classifier(data, content_col, label_col, test_size=0.2, random_state=42):\n", "    data['tokens'] = data[content_col].apply(tokenize)\n", "    data['tokens'] = data['tokens'].apply(lambda x: ' '.join(x))\n", "    \n", "    # Split data\n", "    X_train, X_test, y_train, y_test = train_test_split(data['tokens'], data[label_col], test_size=test_size, random_state=random_state)\n", "    vectorizer = TfidfVectorizer()\n", "    X_train = vectorizer.fit_transform(X_train).toarray()  \n", "    X_test = vectorizer.transform(X_test).toarray()       \n\n", "    # Standardize features\n", "    scaler = StandardScaler()\n", "    X_train = scaler.fit_transform(X_train)\n", "    lda = LinearDiscriminantAnalysis()\n", "    lda.fit(X_train, y_train)\n", "    y_pred = lda.predict(X_test)\n", "    accuracy = accuracy_score(y_test, y_pred)\n", "    print(f\"LDA Accuracy: {accuracy:.2f}\")\n", "    return accuracy"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n", "def qda_classifier(data, content_col, label_col, test_size=0.2, random_state=42):\n", "    data['tokens'] = data[content_col].apply(tokenize)\n", "    data['tokens'] = data['tokens'].apply(lambda x: ' '.join(x))\n", "    X_train, X_test, y_train, y_test = train_test_split(data['tokens'], data[label_col], test_size=test_size, random_state=random_state)\n", "    vectorizer = TfidfVectorizer()\n", "    X_train = vectorizer.fit_transform(X_train).toarray()  \n", "    X_test = vectorizer.transform(X_test).toarray()   \n", "    scaler = StandardScaler()\n", "    X_train = scaler.fit_transform(X_train)\n", "    X_test = scaler.transform(X_test)\n\n", "    # Train QDA\n", "    qda = QuadraticDiscriminantAnalysis()\n", "    qda.fit(X_train, y_train)\n", "    y_pred = qda.predict(X_test)\n", "    accuracy = accuracy_score(y_test, y_pred)\n", "    print(f\"QDA Accuracy: {accuracy:.2f}\")\n", "    return accuracy"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.neural_network import MLPClassifier\n", "def neural_network_classifier(data, content_col, label_col, test_size=0.2, random_state=42):\n", "    \"\"\"\n", "    Reusable function for neural network classification\n", "    \n", "    Args:\n", "    data: pandas DataFrame\n", "    content_col: column name containing text data\n", "    label_col: column name containing labels\n", "    test_size: proportion of data to use for testing\n", "    Returns:\n", "    accuracy: accuracy of classifier\n", "    \"\"\"\n", "    data['tokens'] = data[content_col].apply(tokenize)\n", "    data['tokens'] = data['tokens'].apply(lambda x: ' '.join(x))\n", "    \n", "    # Split data\n", "    X_train, X_test, y_train, y_test = train_test_split(data['tokens'], data[label_col], test_size=test_size, random_state=random_state)\n\n", "    # Vectorize text data using TF-IDF\n", "    vectorizer = TfidfVectorizer()\n", "    X_train = vectorizer.fit_transform(X_train).toarray()  # Convert sparse matrix to dense\n", "    X_test = vectorizer.transform(X_test).toarray()        # Convert sparse matrix to dense\n\n", "    # Standardize features\n", "    scaler = StandardScaler()\n", "    X_train = scaler.fit_transform(X_train)\n", "    X_test = scaler.transform(X_test)\n\n", "    # Train neural network\n", "    nn = MLPClassifier()\n", "    nn.fit(X_train, y_train)\n", "    y_pred = nn.predict(X_test)\n\n", "    # Evaluate accuracy\n", "    accuracy = accuracy_score(y_test, y_pred)\n", "    print(f\"Neural Network Accuracy: {accuracy:.2f}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.base import BaseEstimator, ClassifierMixin\n", "from sklearn.metrics import accuracy_score"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def hybrid_knn_naive_bayes(data, content_col, label_col, test_size=0.2, random_state=42, weight_nb=0.6, weight_knn=0.4, n_neighbors=5):\n", "    data['tokens'] = data[content_col].apply(tokenize)\n", "    data['tokens'] = data['tokens'].apply(lambda x: ' '.join(x))\n", "    X_train, X_test, y_train, y_test = train_test_split(\n", "        data['tokens'], data[label_col], test_size=test_size, random_state=random_state\n", "    )\n", "    vectorizer = TfidfVectorizer()\n", "    X_train_vectorized = vectorizer.fit_transform(X_train)\n", "    X_test_vectorized = vectorizer.transform(X_test)\n\n", "    #NB\n", "    nb_model = MultinomialNB()\n", "    nb_model.fit(X_train_vectorized, y_train)\n", "    nb_probs = nb_model.predict_proba(X_test_vectorized)\n\n", "    #KNN\n", "    knn_model = KNeighborsClassifier(n_neighbors=n_neighbors)\n", "    knn_model.fit(X_train_vectorized, y_train)\n", "    knn_probs = knn_model.predict_proba(X_test_vectorized)\n\n", "    # Combine predictions using weighted voting\n", "    combined_probs = (weight_nb * nb_probs) + (weight_knn * knn_probs)\n", "    y_pred = combined_probs.argmax(axis=1)\n", "    accuracy = accuracy_score(y_test, y_pred)\n", "    print(f\"Hybrid Model Accuracy: {accuracy}\")\n", "    print(\"\\nClassification Report:\")\n", "    print(classification_report(y_test, y_pred))\n", "    def plot_model_contributions(weight_nb, weight_knn):\n", "        plt.figure(figsize=(6, 5))\n", "        plt.bar(['Naive Bayes', 'KNN'], [weight_nb, weight_knn], color=['skyblue', 'lightgreen'])\n", "        plt.ylabel('Weight Contribution')\n", "        plt.title('Hybrid Model Contributions')\n", "        plt.ylim(0, 1)\n", "        plt.tight_layout()\n", "        plt.show()\n", "    plot_model_contributions(weight_nb, weight_knn)\n", "    return accuracy"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def hybrid_nb_lr(data, content_col, label_col, test_size=0.2, random_state=42):\n", "    \"\"\"\n", "    Hybrid model combining Naive Bayes and Logistic Regression.\n", "    \"\"\"\n", "    data['tokens'] = data[content_col].apply(lambda x: ' '.join(tokenize(x)))\n", "    X_train, X_test, y_train, y_test = train_test_split(data['tokens'], data[label_col], test_size=test_size, random_state=random_state)\n", "    \n", "    vectorizer = TfidfVectorizer()\n", "    X_train_vec = vectorizer.fit_transform(X_train)\n", "    X_test_vec = vectorizer.transform(X_test)\n", "    \n", "    # nb\n", "    nb = MultinomialNB()\n", "    nb.fit(X_train_vec, y_train)\n", "    nb_train_probs = nb.predict_proba(X_train_vec)\n", "    nb_test_probs = nb.predict_proba(X_test_vec)\n", "    \n", "    # logistic on nb\n", "    lr = LogisticRegression()\n", "    lr.fit(nb_train_probs, y_train)\n", "    y_pred = lr.predict(nb_test_probs)\n", "    \n", "    accuracy = accuracy_score(y_test, y_pred)\n", "    print(f\"Hybrid NB + LR Accuracy: {accuracy}\")\n", "    print(\"\\nClassification Report:\")\n", "    print(classification_report(y_test, y_pred))\n", "    \n", "    return accuracy"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def neural_network_classifier(data, content_col, label_col, test_size=0.2, random_state=42):\n", "    data['tokens'] = data[content_col].apply(tokenize)\n", "    data['tokens'] = data['tokens'].apply(lambda x: ' '.join(x))\n", "    X_train, X_test, y_train, y_test = train_test_split(data['tokens'], data[label_col], test_size=test_size, random_state=random_state)\n", "    vectorizer = TfidfVectorizer()\n", "    X_train = vectorizer.fit_transform(X_train).toarray()  \n", "    X_test = vectorizer.transform(X_test).toarray()        \n", "    scaler = StandardScaler()\n", "    X_train = scaler.fit_transform(X_train)\n", "    X_test = scaler.transform(X_test)\n\n", "    # use MLP from sklearn\n", "    nn = MLPClassifier()\n", "    nn.fit(X_train, y_train)\n", "    y_pred = nn.predict(X_test)\n", "    accuracy = accuracy_score(y_test, y_pred)\n", "    print(f\"Neural Network Accuracy: {accuracy}\")\n", "    return accuracy"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import tensorflow as tf\n", "from tensorflow.keras.preprocessing.text import Tokenizer\n", "from tensorflow.keras.preprocessing.sequence import pad_sequences\n", "from tensorflow.keras.models import Sequential\n", "from tensorflow.keras.layers import Dense, Dropout\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.metrics import classification_report\n", "import numpy as np"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def neural_network_classifier_tensorflow(data, content_col, label_col, test_size=0.2, random_state=42, max_words=5000, max_len=100):\n", "    # Tokenize text data not using TF-IDF, instead using Tokenizer from Keras\n", "    tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n", "    tokenizer.fit_on_texts(data[content_col])\n", "    sequences = tokenizer.texts_to_sequences(data[content_col])\n", "    padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')\n", "    X_train, X_test, y_train, y_test = train_test_split(padded_sequences, data[label_col], test_size=test_size, random_state=random_state)\n", "    y_train = np.array(y_train)\n", "    y_test = np.array(y_test)\n", "    model = Sequential([\n", "        Dense(128, activation='relu', input_shape=(max_len,)),\n", "        Dropout(0.5),\n", "        Dense(64, activation='relu'),\n", "        Dropout(0.3),\n", "        Dense(1, activation='sigmoid')  # For binary classification\n", "    ])\n", "    model.compile(optimizer='adam',\n", "                  loss='binary_crossentropy',\n", "                  metrics=['accuracy'])\n", "    history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test), verbose=1)\n", "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n", "    print(f\"Neural Network Accuracy: {test_accuracy}\")\n", "    y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n", "    print(\"\\nClassification Report:\")\n", "    print(classification_report(y_test, y_pred))\n", "    return test_accuracy, history"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def plot_training_history(history, model_name='Neural Network'):\n", "    plt.figure(figsize=(12, 5))\n\n", "    # Plot accuracy\n", "    plt.subplot(1, 2, 1)\n", "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n", "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n", "    plt.title(f'{model_name} Accuracy')\n", "    plt.xlabel('Epochs')\n", "    plt.ylabel('Accuracy')\n", "    plt.legend()\n\n", "    # Plot loss\n", "    plt.subplot(1, 2, 2)\n", "    plt.plot(history.history['loss'], label='Train Loss')\n", "    plt.plot(history.history['val_loss'], label='Validation Loss')\n", "    plt.title(f'{model_name} Loss')\n", "    plt.xlabel('Epochs')\n", "    plt.ylabel('Loss')\n", "    plt.legend()\n", "    plt.tight_layout()\n", "    plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["un classifiers"]}, {"cell_type": "markdown", "metadata": {}, "source": ["rint accuracies"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["accuracies = {}\n", "accuracies['albanian'] = {}\n", "accuracies['soccer'] = {}\n", "accuracies['twitter'] = {}"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["accuracies['albanian']['naive_bayes'] = naive_bayes_classifier_count(albanian, 'content', 'fake_news')\n", "accuracies['albanian']['knn'] = knn_classifier(albanian, 'content', 'fake_news')\n", "accuracies['albanian']['svm'] = svm_classifier(albanian, 'content', 'fake_news')\n", "accuracies['albanian']['decision_tree'] = decision_tree_classifier(albanian, 'content', 'fake_news')\n", "accuracies['albanian']['random_forest'] = random_forest_classifier(albanian, 'content', 'fake_news')\n", "accuracies['albanian']['logistic_regression'] = logistic_regression_classifier(albanian, 'content', 'fake_news')\n", "accuracies['albanian']['knn/naive_bayes'] = hybrid_knn_naive_bayes(albanian, 'content', 'fake_news')\n", "accuracies['albanian']['nb/lr'] = hybrid_nb_lr(albanian, 'content', 'fake_news')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["accuracies['soccer']['naive_bayes'] = naive_bayes_classifier_count(soccer, 'tweet', 'real')\n", "accuracies['soccer']['knn'] = knn_classifier(soccer, 'tweet', 'real')\n", "#accuracies['soccer']['svm'] = svm_classifier(soccer, 'tweet', 'real')\n", "#accuracies['soccer']['decision_tree'] = decision_tree_classifier(soccer, 'tweet', 'real')\n", "#accuracies['soccer']['random_forest'] = random_forest_classifier(soccer, 'tweet', 'real')\n", "accuracies['soccer']['logistic_regression'] = logistic_regression_classifier(soccer, 'tweet', 'real')\n", "accuracies['soccer']['knn/naive_bayes'] = hybrid_knn_naive_bayes(soccer, 'tweet', 'real')\n", "accuracies['soccer']['nb/lr'] = hybrid_nb_lr(soccer, 'tweet', 'real')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["soccer_small = soccer.sample(1000)\n", "accuracies['soccer']['svm'] = svm_classifier(soccer_small, 'tweet', 'real')\n", "accuracies['soccer']['decision_tree'] = decision_tree_classifier(soccer_small, 'tweet', 'real')\n", "accuracies['soccer']['random_forest'] = random_forest_classifier(soccer_small, 'tweet', 'real')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["est neural nets"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["accuracy_nn = neural_network_classifier(albanian, 'content', 'fake_news')\n", "accuracy_tf, history_tf = neural_network_classifier_tensorflow(albanian, 'content', 'fake_news')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plot_training_history(history_tf, model_name='Neural Network - TensorFlow (Albanian)')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["accuracies = pd.DataFrame(accuracies)\n", "print(accuracies)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(albanian.shape)\n", "print(soccer.shape)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}